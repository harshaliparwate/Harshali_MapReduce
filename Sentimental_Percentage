import java.io.BufferedReader;
import java.io.IOException;
import java.io.InputStreamReader;
import java.net.URI;
import java.util.HashMap;
import java.util.Map;
import java.util.StringTokenizer;

import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.conf.Configured;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.NullWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.Mapper.Context;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.util.Tool;




public class Sentiment_feedback  {
	
	
	public static class MyMapper extends Mapper<LongWritable,Text, Text, IntWritable> {
        
		
		private Map<String, String> abMap = new HashMap<String, String>();
		
		private Text outputKey = new Text();
		private IntWritable outputValue = new IntWritable();
		String myword = "";
		int myValue=0;
		
		protected void setup(Context context) throws java.io.IOException, InterruptedException{
			
			super.setup(context);

		    URI[] files = context.getCacheFiles(); // getCacheFiles returns null

		    Path p = new Path(files[0]);
		
		    FileSystem fs = FileSystem.get(context.getConfiguration());		    
		
			if (p.getName().equals("AFINN.txt")) {
				
					BufferedReader reader = new BufferedReader(new InputStreamReader(fs.open(p)));

					String line = reader.readLine();
					while(line != null) {
						String[] tokens = line.split("\t");
						String disctionary_word = tokens[0];
						String disctionary__val = tokens[1];
						abMap.put(disctionary_word, disctionary__val);
						line = reader.readLine();
					}
					reader.close();
				}
			
			if (abMap.isEmpty()) {
				throw new IOException("MyError:Unable to load afinn data.");
			}

		
		}	
		
		public void map(LongWritable key, Text value, Context context) throws IOException, InterruptedException {
			  StringTokenizer itr = new StringTokenizer(value.toString());
			 String myword = "";
				int myValue=0;
				
			  while (itr.hasMoreTokens()) {
				   myword = itr.nextToken().toLowerCase();
				   
				   if(abMap.get(myword)!=null)
				   {
					   myValue=Integer.parseInt(abMap.get(myword));
					   if(myValue>0)
					   {
						   myword="positive";
					   }
					   
					   if(myValue<0)
					   {
						   myword="negative"; 
						   myValue=myValue * -1;
						  
					   }
				   }
				   else
				   {
					   myword="positive";
					   myValue=0;
				   }
				   
				   outputKey.set(myword);
				   outputValue.set(myValue);
		        context.write(outputKey, outputValue);
			    
			  }
	
	}
	}	
	
	
	 public static class ReduceClass extends Reducer<Text,IntWritable,NullWritable,Text>
	   {
		    private Text result = new Text();
		    int pos_total=0;
	    	int neg_total=0;
	    	double sentpercent=0.00;
	    	
		    
		    public void reduce(Text key, Iterable<IntWritable> values,Context context) throws IOException, InterruptedException
		    {
		    	int sum=0;
		    	for (IntWritable val:values)
		   	{
		    	sum+=val.get();	
		    	}
		    	if(key.toString().equals("positive"))
		    	{
		    		pos_total=sum;
		    	}
		    	
		    	if(key.toString().equals("negative"))
		    	{
		    		neg_total=sum;
		    	}
		   }
		    
		    
		    protected void cleanup(Context context)throws IOException,InterruptedException
		    {
		    	sentpercent=(((double)pos_total)-((double)neg_total))/(((double)pos_total)+((double)neg_total))*100;
		    	String str="sentiment percentage for given text is "+String.format("%f", sentpercent);
		    	context.write(NullWritable.get(), new Text(str));
		    	
		    	    	
		    }
		    
	   }
	
		
		 public static void main(String[] args) 
	             throws IOException, ClassNotFoundException, InterruptedException {

			 	Configuration conf = new Configuration();
			 	//conf.set("mapreduce.output.textoutputformat.separator", ",");
			 	Job job = Job.getInstance(conf);
				job.setJarByClass(Sentiment_feedback.class);
				job.setJobName("Map Side Join");
				job.setMapperClass(MyMapper.class);
				job.addCacheFile(new Path(args[1]).toUri());
				//job.setPartitionerClass(CaderPartitioner.class);
			     job.setReducerClass(ReduceClass.class);
			   // job.setNumReduceTasks(0);
				job.setMapOutputKeyClass(Text.class);
				job.setMapOutputValueClass(IntWritable.class);
				job.setOutputKeyClass(NullWritable.class);
				job.setOutputValueClass(Text.class);
				FileInputFormat.addInputPath(job, new Path(args[0]));
				FileOutputFormat.setOutputPath(job, new Path(args[2]));
				
				job.waitForCompletion(true);
				
	}
}

		
