

import java.io.BufferedReader;
import java.io.InputStreamReader;
import java.io.IOException;
import java.net.URI;
import java.util.HashMap;
import java.util.Map;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.FileSystem;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.LongWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Partitioner;
import org.apache.hadoop.mapreduce.Reducer;



public class MapSideJoin_EX3  {
	
	
	public static class MyMapper extends Mapper<LongWritable,Text, Text, Text> {
        
		
		private Map<String, String> abMap = new HashMap<String, String>();
		
		private Text outputKey = new Text();
		private Text outputValue = new Text();
		
		protected void setup(Context context) throws java.io.IOException, InterruptedException{
			
			super.setup(context);

		    URI[] files = context.getCacheFiles(); // getCacheFiles returns null

		    Path p = new Path(files[0]);
		
		    FileSystem fs = FileSystem.get(context.getConfiguration());		    
		
			if (p.getName().equals("store_master")) {
				
					BufferedReader reader = new BufferedReader(new InputStreamReader(fs.open(p)));

					String line = reader.readLine();
					while(line != null) {
						String[] tokens = line.split(",");
						String store_id = tokens[0];
						String state = tokens[2];
						abMap.put(store_id, state);
						line = reader.readLine();
					}
					reader.close();
				}
			
			if (abMap.isEmpty()) {
				throw new IOException("MyError:Unable to load store_matser data.");
			}

		}

		
        protected void map(LongWritable key, Text value, Context context)
            throws java.io.IOException, InterruptedException {
        	
        	
        	String row = value.toString();//reading the data from Employees.txt
        	String[] tokens = row.split(",");
        	String p_id=tokens[1];
        	String store_id = tokens[0];
        	String state = abMap.get(store_id);
        	String qty=tokens[2];
        	String price=tokens[3];
        	
        	String total_state = qty+','+price + "," + state; 
        	outputKey.set(p_id);
        	outputValue.set(total_state);
      	  	context.write(outputKey,outputValue);
        }  
}
	
	
	public static class ReduceClass extends Reducer<Text,Text,Text,Text>
	   {
		    private Text result = new Text();
		    
		    public void reduce(Text key, Iterable<Text> values,Context context) throws IOException, InterruptedException
		    {
		    	int sum=0;
		    	String state="'";
		    	String total=" ";
		    	int total1=0;
		    	for(Text val:values)
		    	{
		    	String str[]=val.toString().split(",");
		    	
		    	int qty=Integer.parseInt(str[0]);
		    	sum+=qty;
		    	int price=Integer.parseInt(str[1]);
		    	
		    	total1=sum*price;
		    	
		    	total=Integer.toString( total1);
		    	state=str[2];
	    	   
		    	}
		    	String val1=total+','+state;
		    	 result.set(new Text(val1));
		    		context.write(key, result);
		      //context.write(key, new LongWritable(sum));
		    	
		    }
	   }

	 
	 //partitioner calss
	 public static class CaderPartitioner extends
	   Partitioner < Text, Text >
	   {
	      @Override
	      public int getPartition(Text key, Text value, int numReduceTasks)
	      {
	         String[] str = value.toString().split(",");
	        String state=str[2].trim();
	        if(state.equals("MAH"))
	        {
	            return 0 % numReduceTasks ;
	        }
	         else 
	         {
	            return 1 % numReduceTasks  ;
	         }
	      }
	   }
	
	 public static void main(String[] args) 
             throws IOException, ClassNotFoundException, InterruptedException {

		 	Configuration conf = new Configuration();
		 	conf.set("mapreduce.output.textoutputformat.separator", ",");
		 	Job job = Job.getInstance(conf);
			job.setJarByClass(MapSideJoin_EX3.class);
			job.setJobName("Map Side Join");
			job.setMapperClass(MyMapper.class);
			job.addCacheFile(new Path(args[1]).toUri());
			job.setPartitionerClass(CaderPartitioner.class);
		      job.setReducerClass(ReduceClass.class);
		    job.setNumReduceTasks(2);
			job.setMapOutputKeyClass(Text.class);
			job.setMapOutputValueClass(Text.class);
			
			FileInputFormat.addInputPath(job, new Path(args[0]));
			FileOutputFormat.setOutputPath(job, new Path(args[2]));
			
			job.waitForCompletion(true);
			

}
	 
	
	
 
}
